# Tiny NN 

In this project, I will build a simple NN module to train on toy problems. This will build upon the Andrej Karpathy's `micrograd` NN. To this I want to add some of my own features:

1. A way to have different activation functions, e.g. tanh, relu, sigmoid,...
2. Different loss functions 
3. A way to compile and train the model neatly 
4. Implimenting early stopping and dropout regularisation  
5. Using numpy instead of list to increase efficancy?
