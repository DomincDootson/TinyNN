# Tiny NN 

In this project, I will build a tiny NN module to train on toy problems. This will build upon the Andrej Karpathy's `micrograd` NN. To this I want to add some of my own features:

1. *Done*: A way to have different activation functions, e.g. tanh, relu, sigmoid,...
2. *To Do*: Different loss functions 
3. *To Do*: A way to compile and train the model neatly 
4. *To Do*: Implimenting early stopping and dropout regularisation  
5. *To Do*: Using numpy instead of list to increase efficancy?



### Ref
- A. Karpathy's repo: https://github.com/karpathy/micrograd/tree/master. 
